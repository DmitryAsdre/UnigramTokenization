{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "import itertools\n",
    "\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding tokenizer for initial tokenizer in UnigramModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "    def __init__(self, vocab_size):\n",
    "        self.word_freq = defaultdict(lambda : 0)\n",
    "        self.vocab = []\n",
    "        self.vocab_freqs = defaultdict(lambda : 0)\n",
    "        self.merges = {}\n",
    "        \n",
    "        self.smallest_vocab = []\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_symbol = \"Ä \"\n",
    "    \n",
    "    def pre_tokenize_str(self, text):\n",
    "        text_tokenized_with_spaces = [[[' '] + nltk.word_tokenize(w)] if idx != 0 else [nltk.word_tokenize(w)]  for idx, w in enumerate(text.split(' '))]\n",
    "        text_tokenized_with_spaces = list(itertools.chain(*list(itertools.chain(*text_tokenized_with_spaces))))\n",
    "        \n",
    "        for i in range(len(text_tokenized_with_spaces)):\n",
    "            if text_tokenized_with_spaces[i] == ' ':\n",
    "                text_tokenized_with_spaces[i] = self.special_symbol\n",
    "                \n",
    "        tokenized_text = []\n",
    "        i = 0\n",
    "         \n",
    "        while i < len(text_tokenized_with_spaces):\n",
    "            if i < len(text_tokenized_with_spaces) - 1:\n",
    "                if text_tokenized_with_spaces[i] == self.special_symbol and text_tokenized_with_spaces[i + 1] != self.special_symbol:\n",
    "                    tokenized_text.append(self.special_symbol + text_tokenized_with_spaces[i + 1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    tokenized_text.append(text_tokenized_with_spaces[i])\n",
    "                    i += 1\n",
    "            else:\n",
    "                tokenized_text.append(text_tokenized_with_spaces[i])\n",
    "                i += 1\n",
    "                \n",
    "        return tokenized_text        \n",
    "            \n",
    "    def compute_pair_freqs(self, splits):\n",
    "        pair_freqs = defaultdict(lambda : 0)\n",
    "        \n",
    "        for word, freq in self.word_freq.items():\n",
    "            split = splits[word]\n",
    "            \n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            \n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                \n",
    "                pair_freqs[pair] += freq\n",
    "                \n",
    "        return pair_freqs\n",
    "    \n",
    "    def merge_pair(self, a, b, splits):\n",
    "        for word in self.word_freq:\n",
    "            split = splits[word]\n",
    "            \n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            \n",
    "            i = 0\n",
    "            \n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [a + b] + split[i + 2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "                    \n",
    "            splits[word] = split\n",
    "            \n",
    "        return splits\n",
    "    \n",
    "    def train_tokenizer(self, corpus):\n",
    "        for text in tqdm.tqdm(corpus):\n",
    "            words = self.pre_tokenize_str(text)\n",
    "            \n",
    "            for word in words:\n",
    "                self.word_freq[word] += 1\n",
    "            \n",
    "        alphabet = set()\n",
    "        \n",
    "        for word in self.word_freq.keys():\n",
    "            for letter in word:\n",
    "                if letter not in alphabet:\n",
    "                    alphabet.add(letter)\n",
    "                self.vocab_freqs[letter] += 1\n",
    "        \n",
    "        alphabet = list(alphabet)\n",
    "        alphabet.sort()    \n",
    "        self.vocab = alphabet#[\"<|endoftext|>\"] + alphabet\n",
    "        self.smallest_vocab = self.vocab.copy()\n",
    "        \n",
    "        splits = {word : [c for c in word] for word in self.word_freq.keys()}\n",
    "        \n",
    "        prev_vocab_len = len(self.vocab)\n",
    "        \n",
    "        pbar = tqdm.tqdm(total=self.vocab_size)\n",
    "        pbar.update(prev_vocab_len)\n",
    "        \n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pbar.update(len(self.vocab) - prev_vocab_len)\n",
    "            prev_vocab_len = len(self.vocab)\n",
    "        \n",
    "            pair_freqs = self.compute_pair_freqs(splits)\n",
    "        \n",
    "        \n",
    "            max_freq = None\n",
    "            best_pair = ''\n",
    "            for pair, freq in pair_freqs.items():\n",
    "                if max_freq is None or max_freq < freq:\n",
    "                    best_pair = pair\n",
    "                    max_freq = freq\n",
    "        \n",
    "            self.merges[best_pair] = ''.join(best_pair)\n",
    "            self.vocab_freqs[''.join(best_pair)] = max_freq\n",
    "        \n",
    "            splits = self.merge_pair(*best_pair, splits)\n",
    "            \n",
    "            self.vocab.append(best_pair[0] + best_pair[1])\n",
    "            \n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        pre_tokenized_text = self.pre_tokenize_str(text)\n",
    "        \n",
    "        splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "                \n",
    "        for pair, merge in self.merges.items():\n",
    "            i = 0\n",
    "            \n",
    "            for idx, split in enumerate(splits):\n",
    "                i = 0\n",
    "                \n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits[idx] = split\n",
    "                \n",
    "        return sum(splits, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Tokenizer \n",
    "- Viterbi alg.\n",
    "- Hard-EM alg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramTokenizer():\n",
    "    def __init__(self, vocab_size, \n",
    "                 initial_vocab_multiplier,\n",
    "                 shrink_multiplier=0.1,\n",
    "                 sub_em_steps=2):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.initial_vocab_multiplier = initial_vocab_multiplier\n",
    "        self.shrink_multplier = shrink_multiplier\n",
    "        self.sub_em_steps = sub_em_steps\n",
    "        \n",
    "        self.initial_tokenizer = BPETokenizer(int(self.vocab_size*self.initial_vocab_multiplier))\n",
    "        \n",
    "        self.cur_vocab_subword_freqs = None\n",
    "        self.cur_vocab_subword_logprob = None\n",
    "        \n",
    "        self.alphabet = None\n",
    "    \n",
    "    def pre_tokinze_str(self, text):\n",
    "        return self.initial_tokenizer.pre_tokenize_str(text)\n",
    "    \n",
    "    def get_initial_word_freq(self):\n",
    "        return self.initial_tokenizer.word_freq\n",
    "    \n",
    "    def train_initial_tokenizer(self, corpus):\n",
    "        self.initial_tokenizer.train_tokenizer(corpus)\n",
    "        self.alphabet = set(self.initial_tokenizer.smallest_vocab)\n",
    "        \n",
    "        self.cur_vocab_subword_freqs = self.initial_tokenizer.vocab_freqs\n",
    "        tot_cnt = sum(list(self.cur_vocab_subword_freqs.values()))\n",
    "        self.cur_vocab_subword_logprob = {k : np.log(v / tot_cnt) for k, v in self.cur_vocab_subword_freqs.items()}\n",
    "\n",
    "    def get_initial_subword_logprob(self):\n",
    "        vocab_freqs = self.initial_tokenizer.vocab_freqs\n",
    "        tot_cnt = sum(list(vocab_freqs.values()))\n",
    "        subword_logp = {k : np.log(v / tot_cnt) for k, v in vocab_freqs.items()}\n",
    "        return subword_logp\n",
    "          \n",
    "    @staticmethod\n",
    "    def viterbi_forward(word, subword_logp):\n",
    "        best_subw_slices = [None]*(len(word) + 1)\n",
    "        neg_loglik = np.zeros(len(word) + 1)\n",
    "        \n",
    "        for eow in range(1, len(word) + 1):\n",
    "            neg_loglik[eow] = np.inf\n",
    "            \n",
    "            for bow in range(eow):\n",
    "                subw = word[bow:eow]\n",
    "                \n",
    "                if subw in subword_logp:\n",
    "                    logp = subword_logp[subw]\n",
    "                    \n",
    "                    s = neg_loglik[bow] - logp\n",
    "                    if s < neg_loglik[eow]:\n",
    "                        neg_loglik[eow] = s\n",
    "                        best_subw_slices[eow] = (bow, eow)\n",
    "        return neg_loglik, best_subw_slices\n",
    "    \n",
    "    @staticmethod\n",
    "    def viterbi_backward(word, subw_slices, neg_loglik):\n",
    "        subwords = []\n",
    "        subwords_slices = []\n",
    "        \n",
    "        next_slices = subw_slices[-1]\n",
    "        \n",
    "        while next_slices is not None:\n",
    "            subw = word[next_slices[0]:next_slices[1]]\n",
    "            subwords.append(subw)\n",
    "            subwords_slices.append((next_slices[0],next_slices[1]))\n",
    "            next_slices = subw_slices[next_slices[0]]\n",
    "        subwords.reverse()\n",
    "    \n",
    "        return subwords, subwords_slices, neg_loglik[-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_viterbi_path(word, subword_logp):\n",
    "        neg_loglik, best_subw_slices = UnigramTokenizer.viterbi_forward(word, subword_logp)\n",
    "        subwords, subwords_slices, vit_path_loss = UnigramTokenizer.viterbi_backward(word, best_subw_slices, neg_loglik)\n",
    "        \n",
    "        return subwords, subwords_slices, vit_path_loss\n",
    "    \n",
    "    \n",
    "    def run_e_step(self, estimated_logprob):\n",
    "        initial_word_freq = self.get_initial_word_freq()\n",
    "        \n",
    "        viterbi_subword_freq = defaultdict(lambda : 0)\n",
    "        vit_path_loss_full = 0\n",
    "        \n",
    "        for word in initial_word_freq:\n",
    "            word_freq = initial_word_freq[word]\n",
    "            \n",
    "            subwords_v, _, vit_path_loss = UnigramTokenizer.get_viterbi_path(word, estimated_logprob)\n",
    "            vit_path_loss_full += vit_path_loss*word_freq\n",
    "            for subword_v in subwords_v:\n",
    "                viterbi_subword_freq[subword_v] += word_freq\n",
    "        \n",
    "        return  viterbi_subword_freq, vit_path_loss_full\n",
    "    \n",
    "    def run_m_step(self, viterbi_subword_freq):\n",
    "        \n",
    "        tot_cnt = sum(list(viterbi_subword_freq.values()))\n",
    "        viterbi_logprob = {k : np.log(v / tot_cnt) for k, v in viterbi_subword_freq.items()}\n",
    "        \n",
    "        return viterbi_logprob\n",
    "    \n",
    "    \n",
    "    def delta_loss(self, token, estimated_word_freqs, estimated_logprob):\n",
    "        if token not in estimated_word_freqs:\n",
    "            return None, np.inf\n",
    "        \n",
    "        if token in self.alphabet:\n",
    "            return None, -np.inf\n",
    "        \n",
    "        if len(token) == 1:\n",
    "            return None, -np.inf \n",
    "        \n",
    "        most_probable_split = None\n",
    "        most_probable_split_score = None\n",
    "        \n",
    "        token_logprob = estimated_logprob[token]\n",
    "        estimated_logprob[token] = -np.inf\n",
    "        \n",
    "        most_probable_split, _, most_probable_split_score = UnigramTokenizer.get_viterbi_path(token, estimated_logprob)\n",
    "        most_probable_split_score *= -1\n",
    "        \n",
    "        estimated_logprob[token] = token_logprob\n",
    "                    \n",
    "        if most_probable_split_score is None:\n",
    "            return None, -np.inf\n",
    "        \n",
    "        return most_probable_split, \\\n",
    "               most_probable_split_score*estimated_word_freqs[token] - estimated_logprob[token]*estimated_word_freqs[token]\n",
    "               \n",
    "    def rebuid_vocab(self, tokens):\n",
    "        new_subword_freqs = {}\n",
    "        \n",
    "        for token in tokens:\n",
    "            new_subword_freqs[token] = self.cur_vocab_subword_freqs[token]\n",
    "        self.cur_vocab_subword_freqs = new_subword_freqs\n",
    "            \n",
    "        tot_cnt = sum(list(self.cur_vocab_subword_freqs.values()))\n",
    "        self.cur_vocab_subword_logprob = {k : np.log(v / tot_cnt) for k, v in self.cur_vocab_subword_freqs.items()}\n",
    "            \n",
    "               \n",
    "    def train_tokenizer(self, corpus):\n",
    "        self.train_initial_tokenizer(corpus)\n",
    "        \n",
    "        while len(self.cur_vocab_subword_freqs.keys()) > self.vocab_size:\n",
    "            \n",
    "            viterbi_word_freq = self.cur_vocab_subword_freqs\n",
    "            viterbi_logprob = self.cur_vocab_subword_logprob\n",
    "            \n",
    "            for i in range(self.sub_em_steps):\n",
    "                viterbi_word_freq, _ = self.run_e_step(viterbi_logprob)\n",
    "                viterbi_logprob = self.run_m_step(viterbi_word_freq)\n",
    "            viterbi_losses = []\n",
    "\n",
    "            for token in self.cur_vocab_subword_freqs:  \n",
    "                _, delta = self.delta_loss(token, viterbi_word_freq, viterbi_logprob)\n",
    "                viterbi_losses.append((token, delta))\n",
    "                        \n",
    "            viterbi_losses = sorted(viterbi_losses, key=lambda x: x[1])\n",
    "            \n",
    "            viterbi_losses = viterbi_losses[:max(int(len(viterbi_losses)*(1. - self.shrink_multplier)), self.vocab_size)]\n",
    "            tokens = list(map(lambda x: x[0], viterbi_losses))\n",
    "            tokens = set(tokens).union(set(self.alphabet))\n",
    "            tokens = list(tokens)\n",
    "            \n",
    "            self.rebuid_vocab(tokens)\n",
    "            \n",
    "            if len(viterbi_losses) == self.vocab_size:\n",
    "                break\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        words = self.pre_tokinze_str(text)\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            cur_token, _, _ = self.get_viterbi_path(word, self.cur_vocab_subword_logprob)\n",
    "            tokens.extend(cur_token)\n",
    "        \n",
    "        return tokens     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut = UnigramTokenizer(2000, 5, shrink_multiplier=0.05, sub_em_steps=3)\n",
    "bpe = BPETokenizer(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespear_corpus = []\n",
    "\n",
    "with open('./data/tiny_shakespear.txt', 'r') as r:\n",
    "    for i in r:\n",
    "        if i != '' and i != \"\\n\":\n",
    "            shakespear_corpus.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 32777/32777 [00:08<00:00, 4001.10it/s]\n",
      "100%|ââââââââââ| 1999/2000 [00:37<00:00, 53.46it/s]\n"
     ]
    }
   ],
   "source": [
    "bpe.train_tokenizer(shakespear_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 32777/32777 [00:07<00:00, 4124.89it/s]\n",
      "100%|ââââââââââ| 9999/10000 [02:24<00:00, 69.00it/s]\n"
     ]
    }
   ],
   "source": [
    "ut.train_tokenizer(shakespear_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'it', 'ize', 'n', 'Ä understand', 'Ä world', '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut.tokenize('Citizen understand world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'itizen', 'Ä under', 'st', 'and', 'Ä world', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.tokenize('Citizen understand world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'ope',\n",
       " 'ful',\n",
       " 'ly',\n",
       " ',',\n",
       " 'Ä you',\n",
       " 'Ä will',\n",
       " 'Ä be',\n",
       " 'Ä ',\n",
       " 'able',\n",
       " 'Ä to',\n",
       " 'Ä understand',\n",
       " 'Ä how',\n",
       " 'Ä they',\n",
       " 'Ä are',\n",
       " 'Ä tra',\n",
       " 'in',\n",
       " 'ed',\n",
       " 'Ä and',\n",
       " 'Ä g',\n",
       " 'en',\n",
       " 'er',\n",
       " 'ate',\n",
       " 'Ä to',\n",
       " 'k',\n",
       " 'en',\n",
       " 's',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut.tokenize(\"Hopefully, you will be able to understand how they are trained and generate tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'o',\n",
       " 'pe',\n",
       " 'f',\n",
       " 'ull',\n",
       " 'y',\n",
       " ',',\n",
       " 'Ä you',\n",
       " 'Ä will',\n",
       " 'Ä be',\n",
       " 'Ä a',\n",
       " 'ble',\n",
       " 'Ä to',\n",
       " 'Ä under',\n",
       " 'st',\n",
       " 'and',\n",
       " 'Ä how',\n",
       " 'Ä they',\n",
       " 'Ä are',\n",
       " 'Ä tra',\n",
       " 'ined',\n",
       " 'Ä and',\n",
       " 'Ä gener',\n",
       " 'ate',\n",
       " 'Ä to',\n",
       " 'k',\n",
       " 'ens',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.tokenize(\"Hopefully, you will be able to understand how they are trained and generate tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_detect_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
